import fitz  # PyMuPDF
import os
import uuid
import openai
import json
import re
import sys
import logging

from dotenv import load_dotenv
from pathlib import Path

# Get the directory containing extractor.py
SCRIPT_DIR = Path(__file__).resolve().parent
# Assume the project root is one level up from the script's directory
PROJECT_ROOT = SCRIPT_DIR.parent
# Define output directories relative to the project root
OUTPUT_IMAGE_DIR = PROJECT_ROOT / "static" / "extracted_images"
OUTPUT_DATA_DIR = PROJECT_ROOT / "extracted"

# –ó–∞–≥—Ä—É–∑–∫–∞ .env –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ pdf_utils
dotenv_path = SCRIPT_DIR / ".env"
load_dotenv(dotenv_path)

openai.api_key = os.getenv("OPENAI_API_KEY")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

TAG_OPTIONS = [
    "anatomy", "physiology", "pathology", "microbiology", "pharmacology", "radiology", "dental_materials",
    "orthodontics",  "endodontics", "periodontology", "prosthodontics", "oral_surgery", "pedodontics",
    "ethics_law", "infection_control", "medical_emergencies", "diagnostics", "table", "image_caption",
    "formula", "diagram", "reference", "definition", "exam_tip", "case_study", "clinical_guideline", "question_block",
    "card_learning", "card_test", "card_summary,toets", "toetsen", "test", "diagnostic"
]

BLOCK_TYPES = ["text", "table", "formula", "caption", "question"]

mode = "full"
use_ai_structure = False  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ
if len(sys.argv) > 1:
    if sys.argv[1] in ["fast", "full"]:
        mode = sys.argv[1]
    if "ai" in sys.argv:
        use_ai_structure = True
    mode = sys.argv[1]
logging.info(f"üîß –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: {mode}")

def analyze_block_with_gpt(text):
    try:
        prompt = f"""
You are an AI assistant helping to extract content from dental textbooks.
Analyze the following block of text and return a JSON object with:
- "summary": a short 1-sentence summary of the content
- "tags": a list of relevant tags from this set:
  {TAG_OPTIONS}
- "type": one of {BLOCK_TYPES}

Text block:
{text}
"""
        response = openai.chat.completions.create(
            model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å gpt-3.5-turbo
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            response_format={"type": "json_object"} # –Ø–≤–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º JSON
        )
        content = response.choices[0].message.content.strip()
        return json.loads(content)
    except json.JSONDecodeError as e:
        logging.error(f"‚ö†Ô∏è JSONDecodeError –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –±–ª–æ–∫–∞: {e}\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞:\n{response.choices[0].message.content.strip()}")
        return {"summary": "", "tags": [], "type": "text"}
    except Exception as e:
        logging.error(f"‚ö†Ô∏è GPT error: {e}")
        return {"summary": "", "tags": [], "type": "text"}

def is_title_case(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–º (Title Case)."""
    words = text.split()
    if not words:
        return False
    for word in words:
        if len(word) > 1 and not word[0].isupper() and not any(c.isupper() for c in word[1:]):
            return False
        elif len(word) == 1 and not word.isupper(): # –†–∞–∑—Ä–µ—à–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "A.")
            return False
    return True

def is_all_caps(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ—Å—Ç–æ–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤."""
    return text.isupper()

def extract_headings(pdf_path, font_size_threshold=16, top_margin_threshold=150):
    """
    –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–∑ PDF-—Ñ–∞–π–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ —à—Ä–∏—Ñ—Ç–∞, –∂–∏—Ä–Ω–æ—Å—Ç–∏, –ø–æ–ª–æ–∂–µ–Ω–∏—è –∏ —Ñ–æ—Ä–º–∞—Ç–∞ —Ç–µ–∫—Å—Ç–∞.

    Args:
        pdf_path (str): –ü—É—Ç—å –∫ PDF-—Ñ–∞–π–ª—É.
        font_size_threshold (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ (—É–∂–µ—Å—Ç–æ—á–µ–Ω–æ).
        top_margin_threshold (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç –≤–µ—Ä—Ö–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–≤ –ø–∏–∫—Å–µ–ª—è—Ö) –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ (—É–∂–µ—Å—Ç–æ—á–µ–Ω–æ).

    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, –≥–¥–µ –∫–∞–∂–¥—ã–π —Å–ª–æ–≤–∞—Ä—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫
              –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç, –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã.
    """
    headings = []
    try:
        doc = fitz.open(str(pdf_path))
        for page_num in range(len(doc)):
            page = doc[page_num]
            blocks = page.get_text("dict")['blocks']
            base_font_size = None
            # –ü–æ–ø—ã—Ç–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±–∞–∑–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫)
            for block in blocks:
                if 'lines' in block and block['lines']:
                    base_font_size = block['lines'][0]['spans'][0]['size']
                    break
            if base_font_size is None:
                base_font_size = 12 # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å

            for block in blocks:
                if 'lines' in block and len(block['lines']) > 0:
                    is_significantly_larger = False
                    is_bold = False
                    text = ""
                    max_span_size = 0
                    for line in block['lines']:
                        for span in line['spans']:
                            text += span['text']
                            max_span_size = max(max_span_size, span['size'])
                            if span['flags'] & 2**0:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –∂–∏—Ä–Ω–æ–≥–æ —à—Ä–∏—Ñ—Ç–∞
                                is_bold = True

                    if max_span_size > base_font_size * 1.3 or max_span_size > font_size_threshold:
                        is_significantly_larger = True

                    cleaned_text = text.strip()
                    is_title = is_title_case(cleaned_text)
                    is_caps = is_all_caps(cleaned_text)
                    is_short = len(cleaned_text.split()) <= 15 and len(cleaned_text.split('\n')) <= 2

                    # –£–∂–µ—Å—Ç–æ—á–µ–Ω–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    if block['bbox'][1] < top_margin_threshold and is_significantly_larger and is_bold and is_short:
                        headings.append({
                            'text': cleaned_text,
                            'page': page_num + 1,
                            'bbox': block['bbox']
                        })
                    elif block['bbox'][1] < top_margin_threshold and is_significantly_larger and (is_title or is_caps) and is_short:
                        headings.append({
                            'text': cleaned_text,
                            'page': page_num + 1,
                            'bbox': block['bbox']
                        })
                    elif is_significantly_larger and is_bold and is_short and block['bbox'][1] < top_margin_threshold * 2: # –ß—É—Ç—å –º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–æ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–µ –≤ —Å–∞–º–æ–º –≤–µ—Ä—Ö—É
                         headings.append({
                            'text': cleaned_text,
                            'page': page_num + 1,
                            'bbox': block['bbox']
                        })


        return headings

    except Exception as e:
        logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è): {e}")
        return []

def find_toc_pages(pdf_path, search_range=15):
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—ã—Ç–∞–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è –≤ PDF-—Ñ–∞–π–ª–µ,
    –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—è—Å—å –ø–µ—Ä–≤—ã–º–∏ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ `search_range` —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏.

    Args:
        pdf_path (str): –ü—É—Ç—å –∫ PDF-—Ñ–∞–π–ª—É.
        search_range (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Returns:
        list: –°–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ.
              –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å.
    """
    toc_pages = []
    try:
        doc = fitz.open(pdf_path)
        num_pages = len(doc)

        # –ü–æ–∏—Å–∫ –≤ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
        for page_num in range(min(search_range, num_pages)):
            page = doc[page_num]
            text = page.get_text().lower()
            if re.search(r"(table of contents|contents|–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ)", text):
                toc_pages.append(page_num + 1)

        # –ü–æ–∏—Å–∫ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö (–∏–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –µ—Å–ª–∏ –∫–Ω–∏–≥–∞ –∫–æ—Ä–æ—Ç–∫–∞—è)
        if num_pages > search_range:
            for page_num in range(max(0, num_pages - search_range), num_pages):
                page = doc[page_num]
                text = page.get_text().lower()
                if re.search(r"(table of contents|contents|–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ)", text) and (page_num + 1) not in toc_pages:
                    toc_pages.append(page_num + 1)

        return sorted(list(set(toc_pages))) # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º

    except Exception as e:
        logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è: {e}")
        return []

def extract_toc_text(pdf_path, toc_pages):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü PDF-—Ñ–∞–π–ª–∞.

    Args:
        pdf_path (str): –ü—É—Ç—å –∫ PDF-—Ñ–∞–π–ª—É.
        toc_pages (list): –°–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.

    Returns:
        str: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ –≤—Å–µ—Ö —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫.
             –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    toc_text = ""
    try:
        doc = fitz.open(pdf_path)
        for page_num in toc_pages:
            if 1 <= page_num <= len(doc):
                page = doc[page_num - 1]
                toc_text += page.get_text() + "\n\n"
        return toc_text.strip()
    except Exception as e:
        logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è: {e}")
        return ""
    
def analyze_toc_with_gpt(toc_text):

    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é GPT –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    Args:
        toc_text (str): –¢–µ–∫—Å—Ç, –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∏–∑ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è PDF.

    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, –≥–¥–µ –∫–∞–∂–¥—ã–π —Å–ª–æ–≤–∞—Ä—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≥–ª–∞–≤—É
              —Å –ø–æ–ª—è–º–∏ 'title', 'start_page', –∏ –≤–æ–∑–º–æ–∂–Ω–æ 'end_page'.
              –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    try:
        prompt = f"""
–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ —É—á–µ–±–Ω–∏–∫–∞ –ø–æ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏.
–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–∏ –≥–ª–∞–≤—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
–ü–æ–ø—Ä–æ–±—É–π —Ç–∞–∫–∂–µ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–Ω–µ—á–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≥–ª–∞–≤, –µ—Å–ª–∏ —ç—Ç–æ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–æ –∏–ª–∏ –º–æ–∂–Ω–æ –ª–æ–≥–∏—á–µ—Å–∫–∏ –≤—ã–≤–µ—Å—Ç–∏ –∏–∑ —Å–ª–µ–¥—É—é—â–µ–π –≥–ª–∞–≤—ã.
–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –º–∞—Å—Å–∏–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≥–ª–∞–≤—É —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –ø–æ–ª—è–º–∏:
- "title": –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥–ª–∞–≤—ã (—Å—Ç—Ä–æ–∫–∞)
- "start_page": –Ω–æ–º–µ—Ä –Ω–∞—á–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Ü–µ–ª–æ–µ —á–∏—Å–ª–æ)
- "end_page": –Ω–æ–º–µ—Ä –∫–æ–Ω–µ—á–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Ü–µ–ª–æ–µ —á–∏—Å–ª–æ, –µ—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ, –∏–Ω–∞—á–µ null)

–û–≥–ª–∞–≤–ª–µ–Ω–∏–µ:
{toc_text}
"""
        response = openai.chat.completions.create(
            model="gpt-4o",  # –ú–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        content = response.choices[0].message.content.strip()
        return json.loads(content)
    except json.JSONDecodeError as e:
        logging.error(f"‚ö†Ô∏è JSONDecodeError –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è: {e}\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞:\n{response.choices[0].message.content.strip()}")
        return {"chapters": json.loads(content)}
    except Exception as e:
        logging.error(f"‚ö†Ô∏è GPT error –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—è: {e}")
        return []    

def extract_pdf_structure(pdf_path):
    doc = fitz.open(pdf_path)
    all_blocks = []
    # Use the absolute path defined above
    os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True) # Ensure directory exists

    for page_number in range(len(doc)):
        page = doc[page_number]
        blocks = page.get_text("blocks") # Extract blocks with block numbers
        images_on_page = page.get_images(full=True)
        image_bboxes = [page.get_image_bbox(img[0]) for img in images_on_page]

        current_page_images = [] # Store images saved from this page to associate with blocks

        # --- Image Extraction ---
        image_list = page.get_images(full=True)
        for img_index, img in enumerate(image_list):
            xref = img[0]
            try:
                base_image = doc.extract_image(xref)
                if not base_image:
                    logging.warning(f"‚ö†Ô∏è Page {page_number+1}: Skipped image with xref={xref}: base_image is None")
                    continue

                image_bytes = base_image.get("image")
                ext = base_image.get("ext")

                if not image_bytes:
                    logging.warning(f"‚ö†Ô∏è Page {page_number+1}: Skipped image with xref={xref}: no image data")
                    continue

                if not ext:
                    logging.warning(f"‚ö†Ô∏è Page {page_number+1}: Skipped image with xref={xref}: no file extension, defaulting to 'png'")
                    ext = "png" # Default extension if missing

                # Create a more unique filename
                image_filename = f"image_{page_number+1}_{img_index+1}_{uuid.uuid4().hex}.{ext}"
                image_path = OUTPUT_IMAGE_DIR / image_filename
                logging.info(f"üíæ Attempting to save image to: {image_path}") # Log the final absolute path

                try:
                    with open(image_path, "wb") as img_file:
                        img_file.write(image_bytes)
                    current_page_images.append(image_filename)
                    logging.info(f"üì∑ Saved image: {image_filename} (xref={xref}) from page {page_number+1}")
                except IOError as e:
                    logging.error(f"‚ùå Page {page_number+1}: Error saving image {image_filename} (xref={xref}): {e}")
                except Exception as e:
                     logging.error(f"‚ùå Page {page_number+1}: Unexpected error saving image {image_filename} (xref={xref}): {e}")


            except Exception as e:
                logging.error(f"‚ùå Page {page_number+1}: Error processing image with xref={xref}: {e}")
                continue

        # --- Block Processing ---
        for i, block in enumerate(blocks):
            # block format: (x0, y0, x1, y1, "text", block_no, block_type)
            # block_type 0 is text, 1 is image? Check PyMuPDF docs.
            if len(block) < 6: # Need at least coordinates and text
                 logging.warning(f"‚ö†Ô∏è Page {page_number+1}: Skipping block {i} due to unexpected format: {block}")
                 continue

            x0, y0, x1, y1 = block[0], block[1], block[2], block[3]
            text = block[4].strip()
            block_no = block[5] # Get block number
            block_type_num = block[6] # 0 for text, 1 for image

            if block_type_num == 1: # Skip image blocks themselves, we handle images separately
                continue

            if not text: # Skip empty text blocks
                continue

            is_related_to_image = False
            block_bbox = (x0, y0, x1, y1)
            # Check proximity to any image on the page
            for img_bbox in image_bboxes:
                if fitz.Rect(block_bbox).intersects(fitz.Rect(img_bbox)) or \
                   fitz.Rect(block_bbox).is_near(fitz.Rect(img_bbox), distance=20):
                    is_related_to_image = True
                    break

            # *** UNCOMMENTED THIS SECTION ***
            if text:
                gpt_data = analyze_block_with_gpt(text) # Analyze the text block

                # Determine block type more robustly
                final_type = gpt_data.get("type", "text")
                tags = set(gpt_data.get("tags", []))

                # Add heuristic tags/types
                if re.search(r'=|\^|\d+\s*/\s*\d+', text):
                    tags.add("formula")
                    if final_type == "text": final_type = "formula" # Prioritize formula if detected
                if re.search(r'\bcolumn\b|\brow\b|\btable\b|\d+\s*\|\s*\d+', text, re.IGNORECASE):
                    tags.add("table")
                    if final_type == "text": final_type = "table" # Prioritize table
                if re.search(r'(?i)\b(quiz|test|question|choose the correct|exam tip|review question)\b', text):
                    tags.update(["quiz", "question_block", "exam_tip"])
                    if final_type == "text": final_type = "question" # Prioritize question

                # If GPT identified as caption, mark as related to image
                if gpt_data.get("type") == "caption":
                    is_related_to_image = True
                    tags.add("image_caption") # Ensure tag is present

                block_data = {
                    "page": page_number + 1,
                    "text": text,
                    "images": current_page_images[:], # Associate *all* images from the page for now
                    "summary": gpt_data.get("summary", ""),
                    "tags": list(tags),
                    "type": final_type,
                    "block_coords": [x0, y0, x1, y1],
                    "block_id": block_no, # Use the block number from PyMuPDF
                    "is_related_to_image": is_related_to_image
                }
                all_blocks.append(block_data)
            # *** END OF UNCOMMENTED SECTION ***

        # Clear page images list for the next page (optional, good practice)
        # current_page_images = [] # Reset for next page if association should be strictly per-block

    return all_blocks


def analyze_structure_with_ai(blocks):
    try:
        text_by_page = {}
        blocks_by_id = {block['block_id']: block for block in blocks if block['block_id'] != -1} # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –±–ª–æ–∫–æ–≤ –ø–æ ID
        for block in blocks:
            page = block['page']
            if page not in text_by_page:
                text_by_page[page] = []
            text_by_page[page].append(block['text'])

        ordered_text = "\n\n".join([f"[Page {page}]\n" + "\n".join(text_by_page[page]) for page in sorted(text_by_page.keys())])

        prompt = f"""
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç AI, –∫–æ—Ç–æ—Ä—ã–π –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —É—á–µ–±–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏.
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ –≥–ª–∞–≤—ã, –∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–∏, –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É –∏ –≥—Ä–∞–Ω–∏—Ü—ã (–Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü).
–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –º–∞—Å—Å–∏–≤–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –≥–ª–∞–≤—É —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –ø–æ–ª—è–º–∏:
- "title": –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥–ª–∞–≤—ã
- "theme": –æ—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ –≥–ª–∞–≤—ã
- "start_page": –Ω–æ–º–µ—Ä –Ω–∞—á–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
- "end_page": –Ω–æ–º–µ—Ä –∫–æ–Ω–µ—á–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
- "content_block_ids": —Å–ø–∏—Å–æ–∫ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –±–ª–æ–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ (–ø–æ–ª–µ 'block_id' –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö), –æ—Ç–Ω–æ—Å—è—â–∏—Ö—Å—è –∫ —ç—Ç–æ–π –≥–ª–∞–≤–µ.

–¢–µ–∫—Å—Ç:
{ordered_text[:15000]}  # –£–≤–µ–ª–∏—á–∏–º –ª–∏–º–∏—Ç, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
"""
        response = openai.chat.completions.create(
            model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å gpt-3.5-turbo
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            response_format={"type": "json_object"} # –Ø–≤–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º JSON
        )
        content = response.choices[0].message.content.strip()
        structure = json.loads(content)
        if 'chapters' in structure:
            for chapter in structure['chapters']:
                image_names = set()
                has_image_related_content = False
                for block_id in chapter.get('content_block_ids', []):
                    if block_id in blocks_by_id:
                        block = blocks_by_id[block_id]
                        image_names.update(block.get('images', []))
                        if block.get('is_related_to_image', False):
                            has_image_related_content = True
                chapter['image_names'] = list(image_names)
                chapter['has_image_related_content'] = has_image_related_content
        return structure
    except json.JSONDecodeError as e:
        logging.error(f"‚ö†Ô∏è JSONDecodeError –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞:\n{response.choices[0].message.content.strip()}")
        return []
    except Exception as e:
        logging.error(f"‚ö†Ô∏è AI structure analysis failed: {e}")
        return []

def generate_chapter_summary(blocks, chapter_data):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –∫–æ–Ω—Å–ø–µ–∫—Ç –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –≥–ª–∞–≤—ã, –∏—Å–ø–æ–ª—å–∑—É—è GPT-3.5-turbo –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö.

    Args:
        blocks (list): –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF.
        chapter_data (dict): –î–∞–Ω–Ω—ã–µ –æ –≥–ª–∞–≤–µ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –≤–∫–ª—é—á–∞—è 'content_block_ids' –∏ 'image_names'.

    Returns:
        str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç –≥–ª–∞–≤—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    try:
        chapter_text_blocks = [block for block in blocks if block['block_id'] in chapter_data['content_block_ids']]
        chapter_text = "\n\n".join([block['text'] for block in chapter_text_blocks])
        chapter_images = chapter_data.get('image_names', [])
        image_prompt_part = f"\n\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç—Ç–æ–π –≥–ª–∞–≤–æ–π: {', '.join(chapter_images)}" if chapter_images else "\n\n–° —ç—Ç–æ–π –≥–ª–∞–≤–æ–π –Ω–µ —Å–≤—è–∑–∞–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."

        prompt = f"""
–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–∏–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –¥–ª—è —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–æ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏.
–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≥–ª–∞–≤—ã:

{chapter_text}

{image_prompt_part}

—Å–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã, –≤–∫–ª—é—á–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –Ω–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å).
–ö–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ –±–æ–ª–µ–µ 5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
"""
        response = openai.chat.completions.create(
            model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å gpt-3.5-turbo
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logging.error(f"‚ö†Ô∏è GPT error –≤–æ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Å–ø–µ–∫—Ç–∞ –≥–ª–∞–≤—ã (—Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏): {e}")
        return ""

def generate_chapter_cards(blocks, chapter_data):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—É—á–∞—é—â–∏—Ö –∫–∞—Ä—Ç–æ—á–µ–∫ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –≥–ª–∞–≤—ã, —É—á–∏—Ç—ã–≤–∞—è —Å–≤—è–∑—å —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ (—É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç).

    Args:
        blocks (list): –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF.
        chapter_data (dict): –î–∞–Ω–Ω—ã–µ –æ –≥–ª–∞–≤–µ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –≤–∫–ª—é—á–∞—è 'content_block_ids', 'image_names' –∏ 'has_image_related_content'.

    Returns:
        list: –°–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
    """
    try:
        chapter_text_blocks = [block for block in blocks if block['block_id'] in chapter_data['content_block_ids']]
        chapter_text = "\n\n".join([block['text'] for block in chapter_text_blocks])
        chapter_images = chapter_data.get('image_names', [])
        has_image_content = chapter_data.get('has_image_related_content', False)
        image_prompt_part = f"\n\n–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç—Ç–æ–π –≥–ª–∞–≤–æ–π: {', '.join(chapter_images)}" if chapter_images else "\n\n–° —ç—Ç–æ–π –≥–ª–∞–≤–æ–π –Ω–µ —Å–≤—è–∑–∞–Ω—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."
        image_relation_hint = "–í —ç—Ç–æ–π –≥–ª–∞–≤–µ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏." if has_image_content else "–í —ç—Ç–æ–π –≥–ª–∞–≤–µ –Ω–µ—Ç —è–≤–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏."

        prompt = f"""
–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –ø–æ–º–æ–≥–∞—é—â–∏–π —Å–æ–∑–¥–∞–≤–∞—Ç—å –æ–±—É—á–∞—é—â–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤-—Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–æ–≤.
–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≥–ª–∞–≤—ã:

{chapter_text}

{image_prompt_part}
{image_relation_hint}

–ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –≥–ª–∞–≤—ã –∏–ª–∏ –Ω–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —É–∂–µ –≥–æ—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ "Review Questions", "Test Yourself", "Quiz", "–í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–∞–º–æ–∫–æ–Ω—Ç—Ä–æ–ª—è" –∏ —Ç.–ø.), –∏—Å–ø–æ–ª—å–∑—É–π –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å —ç—Ç–∏ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–∏–ø–∞ "test".
–ï—Å–ª–∏ —Ç–∞–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –º–æ–∂–µ—à—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏.

–°–æ–∑–¥–∞–π —Å–ø–∏—Å–æ–∫ –∏–∑ 10‚Äì15 –æ–±—É—á–∞—é—â–∏—Ö –∫–∞—Ä—Ç–æ—á–µ–∫, –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —É–∫–∞–∂–∏:
- "type": –æ–¥–∏–Ω –∏–∑ "learning", "test", "summary"
- "question": —á—Ç–æ –±—É–¥–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–¥–ª—è "test" ‚Äî —Å–∞–º –≤–æ–ø—Ä–æ—Å)
- "answer": –æ—Ç–≤–µ—Ç –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É (–¥–ª—è "test" ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –æ—Ç–≤–µ—Ç–∞, –¥–ª—è "learning" ‚Äî –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ)
- "tags": –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–≥–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –∏–∑ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞: {', '.join(TAG_OPTIONS)}
- "image_hint": —É–∫–∞–∂–∏ true, –µ—Å–ª–∏ –∫–∞—Ä—Ç–æ—á–∫–∞ –Ω–∞–ø—Ä—è–º—É—é –æ—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞, —Å–≤—è–∑–∞–Ω–Ω–æ–≥–æ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, –∏–ª–∏ –µ—Å–ª–∏ –æ–Ω–∞ –æ–±—ä—è—Å–Ω—è–µ—Ç/–∏–ª–ª—é—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤–æ–π –∞—Å–ø–µ–∫—Ç –æ–¥–Ω–æ–≥–æ –∏–∑ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–¥–∞–∂–µ –±–µ–∑ –ø—Ä—è–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞). –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ —É–∫–∞–∂–∏ false. –ü–æ–ø—Ä–æ–±—É–π —Å–æ–∑–¥–∞—Ç—å –∫–∞—Ä—Ç–æ—á–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.

–î–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–∏–ø–∞ "learning": –æ–±—ä—è—Å–Ω—è–π —Ç–µ–º—É –≤ 3‚Äì7 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö, –∫–∞–∫ –µ—Å–ª–∏ –±—ã —Ç—ã –æ–±—É—á–∞–ª —Å—Ç—É–¥–µ–Ω—Ç–∞ —Å –Ω—É–ª—è.
–î–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–∏–ø–∞ "test": —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å —Å 3-4 –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤ (a, b, c, d). –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–µ—Ç–∫–æ —É–∫–∞–∑–∞–Ω –≤ –ø–æ–ª–µ "answer".
–î–ª—è "summary": —Å–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–∏–π –ø–æ–≤—Ç–æ—Ä –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤, –≤–∫–ª—é—á–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ.

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON (–º–∞—Å—Å–∏–≤ –∫–∞—Ä—Ç–æ—á–µ–∫).
"""
        response = openai.chat.completions.create(
            model="gpt-4o",  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å gpt-3.5-turbo
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            response_format={"type": "json_object"} # –Ø–≤–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º JSON
        )
        content = response.choices[0].message.content.strip()
        return json.loads(content)
    except json.JSONDecodeError as e:
        logging.error(f"‚ö†Ô∏è JSONDecodeError –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç–æ—á–µ–∫ –≥–ª–∞–≤—ã (—Å —É—á–µ—Ç–æ–º —Å–≤—è–∑–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏): {e}\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞:\n{response.choices[0].message.content.strip()}")
        return []
    except Exception as e:
        logging.error(f"‚ö†Ô∏è GPT error –≤–æ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—Ä—Ç–æ—á–µ–∫ –≥–ª–∞–≤—ã (—Å —É—á–µ—Ç–æ–º —Å–≤—è–∑–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏): {e}")
        return []
    #------------------------------------------
def generate_learning_cards(blocks):
    return [block for block in blocks if "card_learning" in block.get("tags", [])]

def generate_chapter_summary_from_text(text, chapter_data):
    try:
        prompt = f"""
–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–∏–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã –¥–ª—è —É—á–µ–±–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –ø–æ —Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏–∏.
–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≥–ª–∞–≤—ã —Å–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–∏–π, –Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç, –æ—Ö–≤–∞—Ç—ã–≤–∞—é—â–∏–π –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã.
–ö–æ–Ω—Å–ø–µ–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ –±–æ–ª–µ–µ 5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.

–¢–µ–∫—Å—Ç –≥–ª–∞–≤—ã:
{text}
"""
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logging.error(f"‚ö†Ô∏è GPT error –≤–æ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞ –≥–ª–∞–≤—ã: {e}")
        return ""

def generate_chapter_cards_from_text(text, chapter_data):
    try:
        prompt = f"""
–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –ø–æ–º–æ–≥–∞—é—â–∏–π —Å–æ–∑–¥–∞–≤–∞—Ç—å –æ–±—É—á–∞—é—â–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –¥–ª—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤-—Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥–æ–≤.
–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≥–ª–∞–≤—ã:

{text}

–°–æ–∑–¥–∞–π —Å–ø–∏—Å–æ–∫ –∏–∑ 10‚Äì15 –æ–±—É—á–∞—é—â–∏—Ö –∫–∞—Ä—Ç–æ—á–µ–∫, –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —É–∫–∞–∂–∏:
- "type": –æ–¥–∏–Ω –∏–∑ "learning", "test", "summary"
- "question": —á—Ç–æ –±—É–¥–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
- "answer": –æ—Ç–≤–µ—Ç –Ω–∞ –∫–∞—Ä—Ç–æ—á–∫—É (–¥–ª—è test ‚Äî –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç, –¥–ª—è learning ‚Äî –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ)
- "tags": –∫–ª—é—á–µ–≤—ã–µ —Ç–µ–≥–∏ –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –∏–∑ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞: {', '.join(TAG_OPTIONS)}
- "image_hint": true/false

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON (–º–∞—Å—Å–∏–≤ –∫–∞—Ä—Ç–æ—á–µ–∫).
"""
        response = openai.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content.strip())
    except Exception as e:
        logging.error(f"‚ö†Ô∏è GPT error –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç–æ—á–µ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞: {e}")
        return []


def generate_test_cards(blocks):
    return [block for block in blocks if "card_test" in block.get("tags", [])]

def generate_summary_cards(blocks):
    return [block for block in blocks if "card_summary" in block.get("tags", [])]

def extract_pdf_structure_with_ai(pdf_path):
    return extract_pdf_structure(pdf_path)

def extract_chapter_text(pdf_path, start_page, end_page):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü PDF-—Ñ–∞–π–ª–∞.

    Args:
        pdf_path (str): –ü—É—Ç—å –∫ PDF-—Ñ–∞–π–ª—É.
        start_page (int): –ù–æ–º–µ—Ä –Ω–∞—á–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        end_page (int): –ù–æ–º–µ—Ä –∫–æ–Ω–µ—á–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

    Returns:
        str: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫.
             –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü.
    """
    chapter_text = ""
    try:
        doc = fitz.open(pdf_path)
        try:
            start_page_int = int(start_page)
            end_page_int = int(end_page)
            if 1 <= start_page_int <= end_page_int <= len(doc):
                for page_num in range(start_page_int - 1, end_page_int):  # fitz –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å 0
                    page = doc[page_num]
                    chapter_text += page.get_text() + "\n\n"
            else:
                logging.error(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Å—Ç—Ä–∞–Ω–∏—Ü: {start_page}-{end_page}")
        except ValueError:
            logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ: start_page={start_page}, end_page={end_page}")
        except Exception as e:
            logging.error(f"‚ö†Ô∏è –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü: {e}")
        return chapter_text.strip()
    except Exception as e:
        logging.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ PDF –∏–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≥–ª–∞–≤—ã: {e}")
        return ""

if __name__ == '__main__':
    # Define the input PDF path relative to the project root for testing
    # Ensure the 'uploads' directory exists at the project root
    pdf_input_path = PROJECT_ROOT / "uploads" / "Wheelers_Dental_Anatomy_Physiology_and_Occlusion_11e_Original_PDF.pdf"

    # Check if PDF file exists
    if not pdf_input_path.is_file():
        logging.error(f"‚ùå Input PDF file not found at: {pdf_input_path}")
        sys.exit(1) # Exit if PDF is missing

    # Ensure output directories exist (using absolute paths)
    try:
        os.makedirs(OUTPUT_IMAGE_DIR, exist_ok=True)
        os.makedirs(OUTPUT_DATA_DIR, exist_ok=True)
        logging.info(f"Image Output Dir: {OUTPUT_IMAGE_DIR}")
        logging.info(f"Data Output Dir: {OUTPUT_DATA_DIR}")
    except Exception as e:
        logging.error(f"‚ùå Failed to create output directories: {e}", exc_info=True)
        sys.exit(1) # Exit if directories can't be created

    logging.info(f"Processing PDF: {pdf_input_path}")

    if use_ai_structure:
        logging.info("üß† Using AI structure analysis (extract_pdf_structure + analyze_structure_with_ai)")
        # Pass the absolute path string to the extraction function
        all_blocks = extract_pdf_structure(str(pdf_input_path))
        if not all_blocks:
             logging.error("‚ùå Failed to extract any blocks from PDF. AI structure analysis cannot proceed.")
        else:
            logging.info(f"üìä Extracted {len(all_blocks)} blocks.")
            logging.info("ü§ñ Analyzing structure with AI...")
            structure = analyze_structure_with_ai(all_blocks)
            ai_structure_path = OUTPUT_DATA_DIR / "ai_structure.json"
            try:
                with open(ai_structure_path, "w", encoding="utf-8") as f:
                    json.dump(structure, f, ensure_ascii=False, indent=2)
                logging.info(f"‚úÖ AI structure saved to {ai_structure_path}")
            except Exception as e:
                 logging.error(f"‚ùå Error saving AI structure to {ai_structure_path}: {e}", exc_info=True)

            # --- Process chapters based on AI structure ---
            if structure and 'chapters' in structure and structure['chapters']:
                logging.info(f"ü§ñ AI identified {len(structure['chapters'])} chapters. Processing each...")
                for i, chapter in enumerate(structure['chapters'], 1):
                    title = chapter.get('title', f'AI_Chapter_{i}')
                    safe_title = sanitize_filename(f"ai_{title}") # Sanitize for filename
                    if not safe_title: safe_title = f"AI_Chapter_{i}" # Fallback

                    logging.info(f"--- Processing AI Chapter {i}/{len(structure['chapters'])}: '{title}' ---")

                    block_ids = chapter.get('content_block_ids', [])
                    if not block_ids:
                        logging.warning(f"‚ö†Ô∏è AI Chapter '{title}' has no associated content block IDs. Skipping summary/card generation.")
                        continue

                    # Filter blocks for the current chapter
                    chapter_blocks = [block for block in all_blocks if block.get('block_id') in block_ids]
                    if not chapter_blocks:
                         logging.warning(f"‚ö†Ô∏è AI Chapter '{title}': No blocks found matching IDs {block_ids}. Skipping summary/card generation.")
                         continue

                    logging.info(f"Chapter '{title}': Found {len(chapter_blocks)} associated blocks.")

                    # Generate summary
                    summary_filename = f"summary_{safe_title}.txt"
                    summary_path = OUTPUT_DATA_DIR / summary_filename
                    try:
                        logging.info(f"Generating summary for AI chapter '{title}'...")
                        summary = generate_chapter_summary(chapter_blocks, chapter)
                        if summary:
                            with open(summary_path, "w", encoding="utf-8") as f:
                                f.write(summary)
                            logging.info(f"‚úÖ AI Summary saved to {summary_path}")
                        else:
                            logging.warning(f"‚ö†Ô∏è Generated summary for AI chapter '{title}' was empty.")
                    except Exception as e:
                        logging.error(f"‚ùå Error generating summary for AI chapter '{title}': {e}", exc_info=True)

                    # Generate cards
                    cards_filename = f"cards_{safe_title}.json"
                    cards_path = OUTPUT_DATA_DIR / cards_filename
                    try:
                        logging.info(f"Generating cards for AI chapter '{title}'...")
                        cards_data = generate_chapter_cards(chapter_blocks, chapter) # Expecting a dict/list from JSON
                        if cards_data: # Check if cards_data is not empty
                             # Ensure it's saved as a JSON array if the function returns one directly
                            cards_to_save = cards_data.get('cards', cards_data) if isinstance(cards_data, dict) else cards_data
                            if isinstance(cards_to_save, list):
                                with open(cards_path, "w", encoding="utf-8") as f:
                                    json.dump(cards_to_save, f, ensure_ascii=False, indent=2)
                                logging.info(f"‚úÖ AI Cards saved to {cards_path}")
                            else:
                                 logging.warning(f"‚ö†Ô∏è Card generation for AI chapter '{title}' did not return a list.")
                        else:
                            logging.warning(f"‚ö†Ô∏è Generated cards data for AI chapter '{title}' was empty.")
                    except Exception as e:
                        logging.error(f"‚ùå Error generating cards for AI chapter '{title}': {e}", exc_info=True)
            else:
                 logging.warning("‚ö†Ô∏è AI analysis did not return a valid 'chapters' list in the structure.")

    else: # --- Use Table of Contents (TOC) based approach ---
        logging.info("üìñ Using Table of Contents (TOC) analysis")
        toc_pages = find_toc_pages(str(pdf_input_path))

        if not toc_pages:
            logging.warning("‚ö†Ô∏è Could not automatically find TOC pages. Attempting heading extraction as fallback?")
            # Optionally add heading extraction logic here if TOC fails
            # headings = extract_headings(str(pdf_input_path))
            # if headings: ... process based on headings ...
        else:
            logging.info(f"üìñ Found potential TOC pages: {toc_pages}")
            toc_text = extract_toc_text(str(pdf_input_path), toc_pages)

            if not toc_text:
                logging.warning("‚ö†Ô∏è Failed to extract text from TOC pages.")
            else:
                logging.info("ü§ñ Analyzing TOC text with AI...")
                toc_analysis_result = analyze_toc_with_gpt(toc_text)

                chapter_structure = None
                if isinstance(toc_analysis_result, dict) and 'chapters' in toc_analysis_result:
                     chapter_structure = toc_analysis_result['chapters']
                elif isinstance(toc_analysis_result, list): # Handle case where it might return a list directly
                     chapter_structure = toc_analysis_result

                if not chapter_structure:
                     logging.warning(f"‚ö†Ô∏è Failed to analyze TOC structure using AI. Response: {toc_analysis_result}")
                else:
                    logging.info(f"üìñ TOC analysis identified {len(chapter_structure)} chapters. Processing each...")
                    for i, chapter in enumerate(chapter_structure, 1):
                        title = chapter.get('title', f'TOC_Chapter_{i}')
                        start_page = chapter.get('start_page')
                        end_page = chapter.get('end_page')

                        safe_title = sanitize_filename(f"toc_{title}") # Sanitize for filename
                        if not safe_title: safe_title = f"TOC_Chapter_{i}" # Fallback

                        logging.info(f"--- Processing TOC Chapter {i}/{len(chapter_structure)}: '{title}' (Pages: {start_page}-{end_page}) ---")

                        if start_page is not None and end_page is not None:
                            try:
                                start_page_int = int(start_page)
                                end_page_int = int(end_page)

                                if start_page_int <= 0 or end_page_int < start_page_int:
                                     logging.warning(f"‚ö†Ô∏è Invalid page range for TOC chapter '{title}': {start_page_int}-{end_page_int}. Skipping.")
                                     continue

                                logging.info(f"Extracting text for TOC chapter '{title}' (pages {start_page_int}-{end_page_int})")
                                chapter_text = extract_chapter_text(str(pdf_input_path), start_page_int, end_page_int)
                                logging.info(f"Extracted text length: {len(chapter_text)}")

                                if not chapter_text:
                                    logging.warning(f"‚ö†Ô∏è No text extracted for TOC chapter '{title}'. Skipping summary/card generation.")
                                    continue

                                # Generate summary from text
                                summary_filename = f"summary_{safe_title}.txt"
                                summary_path = OUTPUT_DATA_DIR / summary_filename
                                try:
                                    logging.info(f"Generating summary for TOC chapter '{title}'...")
                                    summary = generate_chapter_summary_from_text(chapter_text, chapter) # Pass chapter data for context if needed
                                    if summary:
                                        with open(summary_path, "w", encoding="utf-8") as f:
                                            f.write(summary)
                                        logging.info(f"‚úÖ TOC Summary saved to {summary_path}")
                                    else:
                                        logging.warning(f"‚ö†Ô∏è Generated summary for TOC chapter '{title}' was empty.")
                                except Exception as e:
                                    logging.error(f"‚ùå Error generating summary for TOC chapter '{title}': {e}", exc_info=True)

                                # Generate cards from text
                                cards_filename = f"cards_{safe_title}.json"
                                cards_path = OUTPUT_DATA_DIR / cards_filename
                                try:
                                    logging.info(f"Generating cards for TOC chapter '{title}'...")
                                    cards_data = generate_chapter_cards_from_text(chapter_text, chapter) # Pass chapter data for context
                                    if cards_data: # Check if cards_data is not empty
                                        # Ensure it's saved as a JSON array
                                        cards_to_save = cards_data.get('cards', cards_data) if isinstance(cards_data, dict) else cards_data
                                        if isinstance(cards_to_save, list):
                                            with open(cards_path, "w", encoding="utf-8") as f:
                                                json.dump(cards_to_save, f, indent=2, ensure_ascii=False)
                                            logging.info(f"‚úÖ TOC Cards saved to {cards_path}")
                                        else:
                                            logging.warning(f"‚ö†Ô∏è Card generation for TOC chapter '{title}' did not return a list.")
                                    else:
                                        logging.warning(f"‚ö†Ô∏è Generated cards data for TOC chapter '{title}' was empty.")
                                except Exception as e:
                                    logging.error(f"‚ùå Error generating cards for TOC chapter '{title}': {e}", exc_info=True)

                            except ValueError:
                                logging.error(f"‚ö†Ô∏è Invalid page numbers for TOC chapter '{title}': start='{start_page}', end='{end_page}'", exc_info=True)
                            except Exception as e:
                                logging.error(f"‚ùå Unexpected error processing TOC chapter '{title}': {e}", exc_info=True)
                        else:
                            logging.warning(f"‚ö†Ô∏è Missing start or end page for TOC chapter '{title}'. Skipping text extraction.")

    logging.info("üèÅ PDF processing finished.")
